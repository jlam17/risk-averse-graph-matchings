from collections import defaultdict
import numpy as np
import time
import warnings

# arbitrary constant
MAX_MATCHING = float('inf')
np.seterr(all='raise')
warnings.filterwarnings('error')

class Hypergraph:
    edge = 'edge'
    weight = 'weight'
    probability = 'probability'
    expected_weight = 'expected_weight'
    variance = 'variance'
    standard_deivation = 'standard_deviation'
    alpha = 'alpha'
    beta = 'beta'
    edge_distribution = 'bernoulli'
    edge_count = 'edges'
    runtime = 'runtime'
    epsilon = 0.01
    variance_beta = False

    def __setattr__(self, name, value):
        # Only allowed pre-defined attributes above to be set
        if hasattr(self, name):
            super(Hypergraph, self).__setattr__(name, value)
        else:
            raise KeyError('Passed a bad key argument')

    def __repr__(self):
        if self.edge_distribution == 'bernoulli':
            edges = len(self.alpha_sorted)
            avg_p1 = np.mean([e[self.weight] for e in self.alpha_sorted])
            avg_p2 = np.mean([e[self.probability] for e in self.alpha_sorted])
            return '{} edges in (hyper)graph w/ {} avg weight {} avg probability'.format(edges, avg_p1, avg_p2)
        elif self.edge_distribution == 'gaussian':
            edges = len(self.alpha_sorted)
            avg_p1 = np.mean([e[self.expected_weight] for e in self.alpha_sorted])
            avg_p2 = np.mean([e[self.variance] for e in self.alpha_sorted])
            return '{} edges in (hyper)graph w/ {} avg weight {} avg probability'.format(edges, avg_p1, avg_p2)
        else:
            return 'Hypergraph has not been defined'

    # TODO: make default weight 1, default prob 1 standard unweighted certain graph
    def __init__(self, edge_list, variance_beta=False, **kwargs):
        """
        Initializes a (hyper)graph for finding bounded-variance matchings

        :param edge_list list: list of dicts defining edges in a (hyper)graph
        :param variance_beta bool: standard deviation (False) or variance (True) beta thresholds
        :param \**kwargs: keyword arguments specifying keys for accessing values in 'edge_list'
        :raises KeyError: specified incorrect key in **kwargs
        :raises ValueError: bad edge values
        """
        # variance or standard dev risk-averse matching
        self.variance_beta = variance_beta

        # change default edge attributes
        for key, value in kwargs.items():
            setattr(self, key, value)

        # check that attributes in 'edge_list' have been defined
        for key in edge_list[0].keys():
            if not hasattr(self, key):
                raise KeyError('Did not define attribute {} in **kwargs'.format(key))

        # Generate expected weight, variance, standard deviation, and alpha
        data = self.__init_attributes(edge_list, self.edge_distribution, variance_beta)

        # data structures for finding matchings
        self.adj_list = defaultdict(lambda: defaultdict(int))
        self.alpha_sorted = sorted(data, key=lambda d: (d[self.alpha], -d[self.probability]), reverse=True)
        self.exp_weight_sorted = sorted(data, key=lambda d: d[self.expected_weight], reverse=True)

    def __init_attributes(self, edges, distrib):
        '''
        Generate following edge attributes based on distribution of edge: expected weight,
        variance, standard deviation, and alpha. Alpha values generated based
        on based on standard deviation or variance

        Gaussian distributed edges should have mean and variance defined.
        Bernoulli distributed edges should have weight and probability defined.

        :param edges list: list of dicts defining edges in a (hyper)graph
        :param distrib str: distribution of edges ('bernoulli' or 'gaussian')
        :return: list of dicts w/ additional generated values
        '''

        if distrib == 'gaussian':
            for entry in edges:
                entry[self.weight], entry[self.probability] = 0, 0
                if entry[self.variance] != 0:
                    beta = entry[self.variance] if self.variance_beta else np.sqrt(entry[self.variance])
                    entry[self.alpha] = entry[self.expected_weight] / beta
                    entry[self.standard_deviation] = self.calc_standard_dev([entry], distrib)
                else:
                    # we slightly perturb zero variance edges. alternatively these edges are
                    # included in all sub-hypergraphs generated by the algorithm, as they never hurt.
                    entry[self.alpha] = float("inf")
                    entry[self.standard_deviation] = 0
        elif distrib == 'bernoulli':
            for entry in edges:
                # we slightly perturb zero weight edges and zero or one probability edges.
                w = self.epsilon if entry[self.weight] == 0 else entry[self.weight]
                p = self.epsilon if entry[self.probability] == 0 \
                    else 1 - self.epsilon if entry[self.probability] == 1 \
                    else entry[self.probability]

                beta = (w**2 * (p * (1 - p))) if self.variance_beta else w * np.sqrt(p * (1 - p))
                entry[self.alpha] = w * p / beta
                entry[self.expected_weight] = entry[self.probability] * entry[self.weight]
                entry[self.standard_deviation] = self.calc_standard_dev([entry], distrib)
                entry[self.variance] = self.calc_variance([entry], distrib)
        else:
            raise ValueError('Only bernoulli and gaussian distributed edges supported')

        return edges

    def print_stats(self, stats, threshold=None):
        stats['probability'] = stats['probability']/stats['edges'] if stats['edges'] > 0 else 0
        if threshold:
            print('{self.beta:.2f} beta threshold {self.variance_beta} variance beta: {self.edge_count} edges, {self.weight} weight, {self.probabililty:.2f} avg probability, {self.expected_weight:2f} exp_weight, {self.standard_deviation:.2f} std, {self.variance:.2f} var {self.runtime:.2f} time'.format(**stats))
        else:
            print('{self.edge_count} edges, {self.weight} weight, {self.probabililty:.2f} avg probability, {self.expected_weight:2f} exp_weight, {self.standard_deviation:.2f} std, {self.variance:.2f} var {self.runtime:.2f} time'.format(**stats))

    def __add_adj_list(self, edges=None):
        '''
        Update adjacency list with specified additional edges (larger subgraph).
        If edges not specified, update adjacency list with all edges (entire graph)

        :param edges list: list of edges to update adjacency list with
        '''
        if edges is None:
            if len(self.adj_list) > 0:
                raise ValueError('Please delete all edges from adjacency list before adding all edges')
            edges = self.exp_weight_sorted

        for edge in edges:
            for author in edge[self.edge]:
                self.adj_list[author]['nodes'] += 1
                self.adj_list[author]['matched'] = False

    def __del_adj_list(self, edges=None):
        '''
        Update adjcancy list by removing specified edges (smaller subgraph)
        If edges not specified, remove all edges from adjacency list

        :param edges list : list of edges to remove from adjacency list
        '''
        if edges is None:
            self.adj_list = defaultdict(lambda: defaultdict(int))
            return
        for edge in edges:
            for author in edge[self.edge]:
                self.adj_list[author]['nodes'] -= 1
                if self.adj_list[author]['nodes'] == 0 :
                    self.adj_list.pop(author)

    def gen_betas(self, intervals):
        '''
        Generate evenly spaced intervals based on the standard deviation

        :param intervals int: number of evenly spaced intervals
        :return: list of (beta) threshold values
        '''
        threshold_vals = None
        _, stats = self.max_matching()
        maxi = int(np.ceil(stats[self.variance])) if self.variance_beta else np.ceil(stats[self.standard_deivation])
        mini = 0
        threshold_vals = [round(val) for val in np.linspace(mini, maxi,intervals+1)]
        print('Generating beta thresholds (variance type {}): {}'.format(self.variance_beta, threshold_vals))
        return threshold_vals

    def __greedy_matching(self, min_alpha, total_edges=None, threshold=None):
        '''
        For all in the current subgraph 'self.adj_list' , find a
        '''
        # greedy matching statistics
        matching_stats = defaultdict(int)
        # total_weight = 0
        # total_prob = 0
        # total_exp_weight = 0
        # total_std = 0
        # total_var = 0

        matching_edges = []
        vertices_removed = []
        exclude_attrib = set([self.alpha, self.edge])
        edge_count = 0
        for e in self.exp_weight_sorted:
            # skip edges with alpha < min alpha of current subgraph
            if min_alpha != MAX_MATCHING and e[self.alpha] <= min_alpha:
                continue
            edge_count += 1
            # skip edge if its standard dev/variance is greater than the current threshold
            beta = e[self.variance] if self.variance_beta else e[self.standard_deviation]
            if threshold and beta > threshold:
                continue
            # check if valid edge; all authors for hyperedge are still available
            available = True
            for author in e[self.edge]:
                if self.adj_list[author]['matched']:
                    available = False
                    break
            if available:
                for e_attrib, val in e.items():
                    if e_attrib in exclude_attrib:
                        continue
                    matching_stats[e_attrib] = val
                matching_edges.append(e)
                # flag vertices that should not be considered
                for author in e[self.edge]:
                    self.adj_list[author]['matched'] = True
                    vertices_removed.append(author)
            # breaks when all valid edges in subgraph have been considered
            # TODO: possible bug here
            if total_edges and edge_count == total_edges:
                break
        # reset adjacency list
        for author in vertices_removed:
            self.adj_list[author]['matched'] = False
        # TODO: delete
        # return matching_edges, len(matching_edges), total_weight, total_prob, total_exp_weight, total_std, total_var
        return matching_edges, len(matching_edges), matching_stats

    def __bounded_matching(self, threshold, threshold_var):
        start = time.time()
        # initialize variables
        hi = len(self.alpha_sorted) - 1
        lo = 0
        mid = (hi + lo)//2
        self.__del_adj_list()
        self.__add_adj_list(self.alpha_sorted[:mid])

        # Binary Search
        while True:
            min_alpha = self.alpha_sorted[mid][self.alpha]
            greedy_matching = self.__greedy_matching(min_alpha, total_edges=mid, threshold=threshold, threshold_var=threshold_var)
            matching_beta = greedy_matching[6] if threshold_var else greedy_matching[5] # variance else standard deviation

            if hi <= mid or lo >= mid:
                break
            elif threshold - matching_beta < 0.1 and threshold - matching_beta > 0:
                break
            elif matching_beta < threshold:
                lo = mid
                mid = (hi+lo)//2
                self.__add_adj_list(self.alpha_sorted[lo:mid])
            else:
                hi = mid
                mid = (hi+lo)//2
                self.__del_adj_list(self.alpha_sorted[mid:hi])
        total_time = time.time() - start
        matching, stats = self.gen_stats_dict(greedy_matching, total_time, threshold)

        #TODO: messy implementation of max output, fix it
        e_next = self.alpha_sorted[mid]
        e_next_stats = [e_next], 1, e_next[self.weight], e_next[self.probability], e_next[self.expected_weight], e_next[self.standard_deviation], e_next[self.variance]
        _, e_next_stats = self.gen_stats_dict(e_next_stats, 0, threshold)
        return max((matching, stats), ([e_next], e_next_stats), key=lambda x: x[1][self.expected_weight])


    def bounded_var_matching(self, threshold):
        return self.__bounded_matching(threshold, True)

    def bounded_std_matching(self, threshold):
        return self.__bounded_matching(threshold, False)

    def max_matching(self):
        '''
        Find a greedy matching on the entire graph
        @returns:
            matching: maximum greedy matching
            stats : dictionary of maximum greedy matching statistics
        '''
        # initialize variables
        self.__del_adj_list()
        self.__add_adj_list()

        start = time.time()
        greedy_matching = self.__greedy_matching(MAX_MATCHING)
        total_time = time.time() - start
        matching, stats = self.gen_stats_dict(greedy_matching, total_time)

        return matching, stats

    def gen_stats_dict(self, greedy_matching, total_time, beta=None):
        '''
        Create a dictionary for stats generated from greedy matching
        @params:
            greedy_matching: result returned from __greedy_matching()
            total_time: runtime to run __greedy_matching()
            bv_input: provide (gamma, beta) tuple for a bounded-variance matching
        '''
        matching, size, weight, prob, exp_weight, std, var = greedy_matching
        stats = {
                'edges': size,
                'weight': weight,
                'probability': prob,
                'expected_weight': exp_weight,
                'std': std,
                'variance': var,
                'runtime': total_time
        }
        if beta is not None:
            stats['variance_beta'] = self.variance_beta
            stats['beta'] = beta
        return matching, stats

    # Standard deviation
    def calc_standard_dev(self, edges, distrib):
        if distrib == 'gaussian':
            return sum(np.sqrt(e[self.variance]) for e in edges) # sqrt(variance)
        elif distrib == 'bernoulli':
            return sum(e[self.weight] * np.sqrt(e[self.probability] * (1-e[self.probability])) for e in edges) # w(sqrt(p(1-p)))
        else:
            raise ValueError("Only gaussian and bernoulli are supported")

    def calc_variance(self, edges, distrib):
        if distrib == 'gaussian':
            return sum(e[self.variance] for e in edges) # variance
        elif distrib == 'bernoulli':
            return sum(e[self.weight]**2 * e[self.probability] * (1-e[self.probability]) for e in edges) # w^2(p(1-p))
        else:
            raise ValueError("Only gaussian and bernoulli are supported")


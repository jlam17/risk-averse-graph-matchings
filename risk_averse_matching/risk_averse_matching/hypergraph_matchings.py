from collections import defaultdict
import numpy as np
import time
import warnings

# arbitrary constant
MAX_MATCHING = float('inf')
np.seterr(all='raise')
warnings.filterwarnings('error')

class Hypergraph:
    def __init__(self, data, prob=None, weight=None, edge='authors', \
                    exp_weight='expected_weight', alpha='alpha', \
                    var='variance', std='standard_deviation', \
                    distrib='bernoulli', epsilon=0.01\
                ):
        '''
        Initializes values and variables needed for finding matchings
        data: list of dictionaries, each dictionary corresponds to a node
        '''
        # TODO: specify edge key
        # Note: make default weight 1, default prob 1 standard unweighted certain graph
        if not (prob or weight):
            raise ValueError("Please indicate the probability and weight key in dict")
        # edge dictionary keys
        # TODO: clean up -> convert into a dict
        self._weight = weight
        self._prob = prob
        self._edge = edge
        self._exp_weight = exp_weight
        self._alpha = alpha
        self._var = var
        self._std = std
        self._distrib = distrib
        self._epsilon = epsilon

        # Generate expected weight, standard deviation, and alpha
        data = self.__init_attributes(data, distrib)

        # data structures
        self.adj_list = defaultdict(lambda: defaultdict(int))
        self.alpha_sorted = sorted(data, key=lambda d: (d[alpha], -d[prob]), reverse = True)
        self.exp_weight_sorted = sorted(data, key=lambda d: d[exp_weight], reverse = True)

        # store max matching in memory
        # TODO: remove after testing is finished
        self.mmatching = None

    def __init_attributes(self, edges, distrib, beta_var=False):
        '''
        generate edge attributes based on distribution of edge
        '''
        # given mean (exp_weight) and variance, generate alpha and standard dev
        if distrib == 'gaussian':
            for entry in edges:
                entry[self._weight] = 0
                entry[self._prob] = 0
                if entry[self._var] != 0:
                    if beta_var:
                        entry[self._alpha] = entry[self._exp_weight]/entry[self._var]
                    else:
                        entry[self._alpha] = entry[self._exp_weight]/np.sqrt(entry[self._var])
                    entry[self._std] = self.calc_standard_dev([entry], distrib)
                    entry[self._var] = self.calc_variance([entry], distrib)
                else:
                    # we slightly perturb zero variance edges, alternatively these edges are included in all sub-hypergraphs generated by the algorithm, as they never hurt.
                    entry[self._alpha] = float("inf")
                    entry[self._std] = 0.0
                    # given weight and probability, generate alpha, exp weight, and standard dev
        elif distrib == 'bernoulli':
            for entry in edges:
                w = self._epsilon if entry[self._weight] == 0 else entry[self._weight]
                if entry[self._prob] == 0:
                    p = self._epsilon
                elif entry[self._prob] == 1:
                    p = 1 - self._epsilon
                else:
                    p = entry[self._prob]
                try:
                    if beta_var:
                        var = (w**2 * (p * (1 - p))) # std = w(sqrt(p(1-p)))
                        entry[self._alpha] = w * p / var # alpha = wp / std
                    else:
                        std = w * np.sqrt(p * (1 - p)) # std = w(sqrt(p(1-p)))
                        entry[self._alpha] = w * p / std # alpha = wp / std
                    entry[self._exp_weight] = entry[self._prob] * entry[self._weight]
                    entry[self._std] = self.calc_standard_dev([entry], distrib)
                    entry[self._var] = self.calc_variance([entry], distrib)
                except FloatingPointError:
                    raise KeyError('Error w/ attribute values. Cannot calculate standard deivation and/or alpha: {}'.format(entry))
        else:
            raise ValueError('init function')
        return edges

    def print_stats(self, stats, threshold=None):
        try:
            if threshold:
                print('{} Beta Threshold: {} edges, {} weight, {} avg probability, {} exp_weight, {} std, {} var {} time'.format(\
                    stats['beta'], stats['edges'], stats['weight'],round(stats['probability']/stats['edges'], 2), \
                    round(stats['expected_weight'], 2), round(stats['std'], 2) , round(stats['variance'], 2) , round(round(stats['runtime'], 2))))
            else:
                print('{} edges, {} weight, {} avg probability, {} exp_weight, {} std, {} var, {} time'.format(\
                    stats['edges'], stats['weight'],  round(stats['probability']/stats['edges'], 2), \
                    round(stats['expected_weight'], 2), round(stats['std'], 2), round(stats['variance'], 2), round(stats['runtime'], 2)))
        except ZeroDivisionError:
            if threshold:
                print('{} Beta Threshold: {} edges, {} weight, {} avg probability, {} exp_weight, {} std, {} var {} time'.format(\
                    stats['beta'], stats['edges'], stats['weight'], stats['edges'], \
                    round(stats['expected_weight'], 2), round(stats['std'], 2) , round(stats['variance'], 2) , round(round(stats['runtime'], 2))))
            else:
                print('{} edges, {} weight, {} avg probability, {} exp_weight, {} std, {} var, {} time'.format(\
                    stats['edges'], stats['weight'],  stats['probability'], \
                    round(stats['expected_weight'], 2), round(stats['std'], 2), round(stats['variance'], 2), round(stats['runtime'], 2)))



    def __add_adj_list(self, edges=None):
        '''
        Update adjacency list with specified additional edges (larger subgraph).
        If edges not specified, update adjacency list with all edges (entire graph)
        @params:
            edges: list of edges to update adjacency list with
        '''
        if edges is None:
            if len(self.adj_list) > 0:
                raise ValueError('Please delete all edges from adjacency list before adding all edges')
            edges = self.exp_weight_sorted

        for edge in edges:
            for author in edge[self._edge]:
                self.adj_list[author]['nodes'] += 1
                self.adj_list[author]['matched'] = False

    def __del_adj_list(self, edges=None):
        '''
        Update adjcancy list by removing specified edges (smaller subgraph)
        If edges not specified, remove all edges from adjacency list
        @params:
            edges: list of edges to remove from adjacency list
        '''
        if edges is None:
            self.adj_list = defaultdict(lambda: defaultdict(int))
            return
        for edge in edges:
            for author in edge[self._edge]:
                self.adj_list[author]['nodes'] -= 1
                if self.adj_list[author]['nodes'] == 0 :
                    self.adj_list.pop(author)

    def gen_betas(self, intervals, beta_var=False):
        '''
        Generate evenly spaced intervals based on the standard deviation
        @params:
            intervals: Number of evenly spaced intervals
        @returns:
            threshold_vals: list of (beta) threshold values
        '''
        threshold_vals = None
        _, stats = self.max_matching()
        maxi = int(np.ceil(stats['std'])) if not beta_var else np.ceil(stats['variance'])
        mini = 0
        threshold_vals = [round(val) for val in np.linspace(mini, maxi,intervals+1)]
        print('Generating beta thresholds {}: {}'.format(beta_var, threshold_vals))
        return threshold_vals

    def __greedy_matching(self, min_alpha, total_edges=None, threshold=None, threshold_var=False, distrib='bernoulli'):
        # stored max matching returned if already found
        # TODO: delete after testing finished
        if min_alpha == MAX_MATCHING and self.mmatching is not None:
            return self.mmatching
        # greedy matching statistics
        total_weight = 0
        total_prob = 0
        total_exp_weight = 0
        total_std = 0
        total_var = 0

        matching_edges = []
        vertex_removed = []
        edge_count = 0
        for e in self.exp_weight_sorted:
            # skip edges with alpha < min alpha of current subgraph
            if min_alpha != MAX_MATCHING and e[self._alpha] <= min_alpha:
                continue

            edge_count += 1
            # skip edge if its standard dev is greater than the current threshold
            edge_std, edge_var = e[self._std], e[self._var]
            if threshold and not threshold_var and edge_std > threshold:
                continue
            elif threshold and threshold_var and edge_var > threshold:
                continue

            # check if valid edge: all authors for hyperedge are still available
            available = True
            for author in e[self._edge]:
                if self.adj_list[author]['matched']:
                    available = False
                    break
            # edge is available for matching
            if available:
                total_weight += e[self._weight]
                total_prob += e[self._prob]
                total_exp_weight += e[self._exp_weight]
                total_std += edge_std
                total_var += edge_var
                matching_edges.append(e)
                # flag vertices that should not be considered
                for author in e[self._edge]:
                    self.adj_list[author]['matched'] = True
                    vertex_removed.append(author)
            # breaks when all valid edges in subgraph have been considered
            # TODO: possible bug here
            if total_edges and edge_count == total_edges:
                break

        # reset adjacency list
        for author in vertex_removed:
            self.adj_list[author]['matched'] = False

        # stores max matching to be reused later
        # TODO: delete after testing finished
        if min_alpha == MAX_MATCHING and self.mmatching is None:
            self.mmatching = matching_edges, len(matching_edges), total_weight, total_prob, total_exp_weight, total_std, total_var

        return matching_edges, len(matching_edges), total_weight, total_prob, total_exp_weight, total_std, total_var

    def __bounded_matching(self, threshold, threshold_var, distrib):
        start = time.time()

        # initialize variables
        hi = len(self.alpha_sorted) - 1
        lo = 0
        mid = (hi + lo)//2
        self.__del_adj_list()
        self.__add_adj_list(self.alpha_sorted[:mid])

        # Binary Search
        while True:
            min_alpha = self.alpha_sorted[mid][self._alpha]
            greedy_matching = self.__greedy_matching(min_alpha, total_edges=mid, threshold=threshold, threshold_var=threshold_var, distrib=distrib)
            matching_beta = greedy_matching[6] if threshold_var else greedy_matching[5] # variance else standard deviation

            if hi <= mid or lo >= mid:
                break
            elif threshold - matching_beta < 0.1 and threshold - matching_beta > 0:
                break
            elif matching_beta < threshold:
                lo = mid
                mid = (hi+lo)//2
                self.__add_adj_list(self.alpha_sorted[lo:mid])
            else:
                hi = mid
                mid = (hi+lo)//2
                self.__del_adj_list(self.alpha_sorted[mid:hi])
        total_time = time.time() - start
        matching, stats = self.gen_stats_dict(greedy_matching, total_time, threshold)

        #TODO: messy implementation of max output, fix it
        e_next = self.alpha_sorted[mid]
        e_next_stats = [e_next], 1, e_next[self._weight], e_next[self._prob], e_next[self._exp_weight], e_next[self._std], e_next[self._var]
        _, e_next_stats = self.gen_stats_dict(e_next_stats, 0, threshold)
        return max((matching, stats), ([e_next], e_next_stats), key=lambda x: x[1][self._exp_weight])


    def bounded_var_matching(self, threshold, distrib='bernoulli'):
        return self.__bounded_matching(threshold, True, distrib)

    def bounded_std_matching(self, threshold, distrib='bernoulli'):
        return self.__bounded_matching(threshold, False, distrib)

    def max_matching(self):
        '''
        Find a greedy matching on the entire graph
        @returns:
            matching: maximum greedy matching
            stats : dictionary of maximum greedy matching statistics
        '''
        # initialize variables
        self.__del_adj_list()
        self.__add_adj_list()

        start = time.time()
        greedy_matching = self.__greedy_matching(MAX_MATCHING)
        total_time = time.time() - start
        matching, stats = self.gen_stats_dict(greedy_matching, total_time)

        return matching, stats

    def gen_stats_dict(self, greedy_matching, total_time, beta=None):
        '''
        Create a dictionary for stats generated from greedy matching
        @params:
            greedy_matching: result returned from __greedy_matching()
            total_time: runtime to run __greedy_matching()
            bv_input: provide (gamma, beta) tuple for a bounded-variance matching
        '''
        matching, size, weight, prob, exp_weight, std, var = greedy_matching
        stats = {
                'edges': size,
                'weight': weight,
                'probability': prob,
                'expected_weight': exp_weight,
                'std': std,
                'variance': var,
                'runtime': total_time
        }
        if beta is not None:
            stats['beta'] = beta
        return matching, stats

    # Standard deviation
    def calc_standard_dev(self, edges, distrib):
        if distrib == 'gaussian':
            return sum(np.sqrt(e[self._var]) for e in edges) # sqrt(variance)
        elif distrib == 'bernoulli':
            return sum(e[self._weight] * np.sqrt(e[self._prob] * (1-e[self._prob])) for e in edges) # w(sqrt(p(1-p)))
        else:
            raise ValueError("Only gaussian and bernoulli are supported")

    def calc_variance(self, edges, distrib):
        if distrib == 'gaussian':
            return sum(e[self._var] for e in edges) # variance
        elif distrib == 'bernoulli':
            return sum(e[self._weight]**2 * e[self._prob] * (1-e[self._prob]) for e in edges) # w^2(p(1-p))
        else:
            raise ValueError("Only gaussian and bernoulli are supported")

